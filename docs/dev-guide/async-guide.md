# Guia de Programa√ß√£o Ass√≠ncrona

Este guia explica como a arquitetura ass√≠ncrona funciona no CreateAgents AI.

---

## üîÑ Por Que Async?

O CreateAgents AI usa programa√ß√£o ass√≠ncrona para:

- **Streaming**: Tokens em tempo real das APIs (OpenAI/Ollama)
- **Tools**: Execu√ß√£o n√£o-bloqueante de ferramentas
- **Performance**: M√∫ltiplas chamadas concorrentes

---

## üéØ Componentes Ass√≠ncronos

### ChatRepository (Interface)

```python
class ChatRepository(ABC):
    @abstractmethod
    async def chat(self, agent: Agent, message: str) -> AsyncGenerator[str, None]:
        """Chat ass√≠ncrono que retorna AsyncGenerator."""
        pass
```

### ChatAdapter (Implementa√ß√£o)

```python
class OpenAIChatAdapter(ChatRepository):
    async def chat(self, agent: Agent, message: str) -> AsyncGenerator[str, None]:
        handler = OpenAIStreamHandler(...)
        async for token in handler.handle_streaming(...):
            yield token
```

### Stream Handlers

#### OpenAIStreamHandler

```python
class OpenAIStreamHandler:
    async def handle_streaming(
        self,
        client,
        model: str,
        messages,
        ...
    ) -> AsyncGenerator[str, None]:
        # Inicia streaming
        stream = client.chat.completions.create(
            model=model,
            messages=messages,
            stream=True,
            ...
        )

        async for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
```

#### OllamaStreamHandler

```python
class OllamaStreamHandler:
    async def handle_streaming(
        self,
        client,
        model: str,
        messages,
        ...
    ) -> AsyncGenerator[str, None]:
        async for response in client.chat(
            model=model,
            messages=messages,
            stream=True,
            ...
        ):
            if response.get('message', {}).get('content'):
                yield response['message']['content']
```

---

## üõ†Ô∏è Execu√ß√£o Ass√≠ncrona de Ferramentas

### ToolExecutor

```python
class ToolExecutor:
    async def execute(
        self,
        tool: BaseTool,
        arguments: Dict[str, Any]
    ) -> ToolExecutionResult:
        self._logger.info("Executing tool: %s", tool.name)

        try:
            # Executa tool (pode ser async ou sync)
            if asyncio.iscoroutinefunction(tool.execute):
                result = await tool.execute(**arguments)
            else:
                result = tool.execute(**arguments)

            return ToolExecutionResult(success=True, result=result)
        except Exception as e:
            return ToolExecutionResult(success=False, error=str(e))
```

---

## üîÑ Fluxo Ass√≠ncrono Completo

### Sem Ferramentas

```
User: await agent.chat("mensagem")
  ‚Üí ChatWithAgentUseCase.execute() [async]
      ‚Üí ChatRepository.chat() [async]
          ‚Üí OpenAIStreamHandler.handle_streaming() [async]
              ‚Üí async for chunk in openai_stream:
                  ‚Üí yield chunk
          ‚Üê AsyncGenerator[str, None]
      ‚Üê StreamingResponseDTO
  ‚Üê await response (string completa)
```

### Com Ferramentas

```
User: await agent.chat("Que dia √© hoje?")
  ‚Üí ChatWithAgentUseCase.execute() [async]
      ‚Üí ChatRepository.chat() [async]
          ‚Üí OpenAIStreamHandler.handle_streaming() [async]
              ‚Üí async for chunk in openai_stream:
                  ‚Üí Detecta tool_calls
              ‚Üí Para cada tool_call:
                  ‚Üí ToolExecutor.execute(tool, args) [async]
                      ‚Üê ToolExecutionResult
              ‚Üí Segunda chamada API com tool results
              ‚Üí async for token in second_stream:
                  ‚Üí yield token
          ‚Üê AsyncGenerator[str, None]
      ‚Üê StreamingResponseDTO
  ‚Üê await response
```

---

## üí° Padr√µes de Uso

### Padr√£o 1: Consumo Simples (Await)

```python
import asyncio

async def simple_chat():
    from createagents import CreateAgent

    agent = CreateAgent(provider="openai", model="gpt-4")
    response = await agent.chat("Ol√°")  # Aguarda string completa
    print(response)

asyncio.run(simple_chat())
```

### Padr√£o 2: Streaming Manual (Async For)

```python
import asyncio

async def streaming_chat():
    from createagents import CreateAgent

    agent = CreateAgent(provider="openai", model="gpt-4")
    response = await agent.chat("Conte uma hist√≥ria")

    async for token in response:
        print(token, end='', flush=True)
    print()

asyncio.run(streaming_chat())
```

### Padr√£o 3: M√∫ltiplas Chamadas Concorrentes

```python
import asyncio

async def concurrent_chats():
    from createagents import CreateAgent

    agent1 = CreateAgent(provider="openai", model="gpt-4")
    agent2 = CreateAgent(provider="openai", model="gpt-4")

    # Executar simultaneamente
    results = await asyncio.gather(
        agent1.chat("Pergunta 1"),
        agent2.chat("Pergunta 2"),
    )

    print(results[0])
    print(results[1])

asyncio.run(concurrent_chats())
```

### Padr√£o 4: Ferramentas Ass√≠ncronas

```python
from createagents import BaseTool
import asyncio
import aiohttp

class AsyncWebTool(BaseTool):
    name = "async_web_fetch"
    description = "Busca dados da web assincronamente"
    parameters = {
        "type": "object",
        "properties": {
            "url": {"type": "string", "description": "URL to fetch"}
        },
        "required": ["url"]
    }

    async def execute(self, url: str) -> str:
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                return await response.text()

# Uso
async def main():
    agent = CreateAgent(
        provider="openai",
        model="gpt-4",
        tools=[AsyncWebTool()]
    )

    response = await agent.chat("Busque dados de https://api.example.com")
    print(await response)

asyncio.run(main())
```

---

## üîß Implementa√ß√£o de Handlers

### Handler N√£o-Streaming

```python
class OpenAIHandler:
    async def handle_non_streaming(
        self,
        client,
        model: str,
        messages: List[dict],
        ...
    ) -> str:
        response = await client.chat.completions.create(
            model=model,
            messages=messages,
            stream=False,
            ...
        )
        return response.choices[0].message.content
```

### Handler com M√©tricas

```python
class OpenAIStreamHandler:
    def __init__(self, ...):
        self._logger = LoggingConfig.get_logger(__name__)
        self._metrics = MetricsRecorder(metrics_list)

    async def handle_streaming(self, ...) -> AsyncGenerator[str, None]:
        start_time = time.time()

        try:
            # Streaming
            async for token in stream:
                yield token

            # Gravar m√©tricas de sucesso
            self._metrics.record_success_metrics(
                model=model,
                start_time=start_time,
                response_api=full_response,
                provider_type='openai'
            )
        except Exception as e:
            # Gravar m√©tricas de erro
            self._metrics.record_error_metrics(
                model=model,
                start_time=start_time,
                error=e
            )
            raise
```

---

## üêõ Armadilhas Comuns

### 1. Esquecer await

```python
# ‚ùå ERRADO
response = agent.chat("mensagem")  # Retorna coroutine
print(response)  # <coroutine object...>

# ‚úÖ CORRETO
response = await agent.chat("mensagem")
print(await response)  # String
```

### 2. Bloquear Loop de Eventos

```python
# ‚ùå ERRADO (blocking I/O)
async def bad_function():
    time.sleep(10)  # Bloqueia todo o loop!

# ‚úÖ CORRETO (non-blocking)
async def good_function():
    await asyncio.sleep(10)  # Permite outras tasks
```

### 3. N√£o Usar asyncio.run()

```python
# ‚ùå ERRADO
async def main():
    response = await agent.chat("mensagem")
    print(await response)

main()  # Erro! Coroutine n√£o executada

# ‚úÖ CORRETO
asyncio.run(main())
```

### 4. Consumir Stream Duas Vezes

```python
# ‚ùå ERRADO
response = await agent.chat("mensagem")
text1 = await response  # Consome stream
text2 = await response  # J√° consumido! text2 = ""

# ‚úÖ CORRETO
response = await agent.chat("mensagem")
text = await response  # Consumir apenas uma vez
```

---

## üìä Performance

### Concorr√™ncia vs Sequencial

**Sequencial**:

```python
async def sequential():
    r1 = await agent.chat("Q1")  # 2s
    r2 = await agent.chat("Q2")  # 2s
    r3 = await agent.chat("Q3")  # 2s
    # Total: 6s
```

**Concorrente**:

```python
async def concurrent():
    results = await asyncio.gather(
        agent.chat("Q1"),  # 2s
        agent.chat("Q2"),  # 2s
        agent.chat("Q3"),  # 2s
    )
    # Total: ~2s (paralelizado)
```

---

## üß™ Testando C√≥digo Ass√≠ncrono

```python
import pytest

@pytest.mark.asyncio
async def test_chat():
    agent = CreateAgent(provider="openai", model="gpt-4")
    response = await agent.chat("Test message")
    text = await response
    assert isinstance(text, str)
    assert len(text) > 0

@pytest.mark.asyncio
async def test_streaming():
    agent = CreateAgent(provider="openai", model="gpt-4")
    response = await agent.chat("Test")

    tokens = []
    async for token in response:
        tokens.append(token)

    assert len(tokens) > 0
```

---

## üí° Best Practices

1. **Sempre use await**: Para executar coroutines
2. **Use asyncio.gather**: Para chamadas concorrentes
3. **N√£o bloqueie**: Use bibliotecas async (aiohttp, aiofiles)
4. **Trate exce√ß√µes**: try/except em c√≥digo async
5. **Logging apropriado**: Use logger em fun√ß√µes async
6. **Teste com pytest-asyncio**: Marque tests com `@pytest.mark.asyncio`

---

## üìö Refer√™ncias

- [Python asyncio](https://docs.python.org/3/library/asyncio.html)
- [Async Generators](https://peps.python.org/pep-0525/)
- [API de Streaming](../reference/streaming-api.md)

---

**Vers√£o:** 0.1.3 | **Atualiza√ß√£o:** 01/12/2025
