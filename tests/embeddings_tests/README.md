# Sistema RAG (Retrieval-Augmented Generation)

Sistema completo de RAG profissional com indexaÃ§Ã£o de documentos, busca vetorial, reranking BM25 e geraÃ§Ã£o de respostas.

## ğŸ“‹ Estrutura

```
embeddings_tests/
â”œâ”€â”€ indexar.py          # Pipeline de indexaÃ§Ã£o de documentos
â”œâ”€â”€ perguntar.py        # Sistema RAG de perguntas e respostas
â”œâ”€â”€ main.py            # Script para criar Ã­ndices
â”œâ”€â”€ test_rag.py        # Script de testes do sistema RAG
â””â”€â”€ README.md          # Esta documentaÃ§Ã£o
```

## ğŸš€ Como Usar

### 1. Instalar DependÃªncias

```bash
poetry install
```

As dependÃªncias necessÃ¡rias incluem:

- `pymupdf` (fitz) - Para processar PDFs
- `pandas` - Para processar CSV/Excel
- `pyarrow` - Para processar Parquet
- `numpy` - OperaÃ§Ãµes numÃ©ricas
- `faiss-cpu` - IndexaÃ§Ã£o vetorial
- `ollama` - API para modelos de embedding e LLM

### 2. Criar Ãndice de Documentos

Edite o arquivo `main.py` para configurar seus documentos:

```python
result = create_embeddings(
    documents=["/caminho/para/seu/documento.pdf"],
    model_name="qwen3-embedding:4b",
    chunk_size=1000,
    chunk_overlap=150,
    batch_size=512,
    num_workers=4,
    output_prefix="meu_index",
)
```

Execute:

```bash
poetry run python tests/embeddings_tests/main.py
```

**Arquivos gerados:**

- `meu_index.faiss` - Ãndice FAISS com embeddings
- `meu_index.jsonl` - Metadados dos chunks
- `meu_index_stats.json` - EstatÃ­sticas do processamento
- `meu_index_metrics.jsonl` - MÃ©tricas de indexaÃ§Ã£o (opcional)

### 3. Fazer Perguntas

Edite o arquivo `test_rag.py` para configurar suas perguntas:

```python
# ConfiguraÃ§Ã£o
INDEX_PREFIX = "meu_index"
EMBEDDING_MODEL = "qwen3-embedding:4b"
LLM_MODEL = "qwen3:latest"

# Perguntas
perguntas = [
    "Sua pergunta aqui?",
    "Outra pergunta?",
]
```

Execute:

```bash
poetry run python tests/embeddings_tests/test_rag.py
```

## ğŸ“Š Funcionalidades

### Sistema de IndexaÃ§Ã£o (`indexar.py`)

#### âœ¨ Principais Features:

1. **Splitter Recursivo Inteligente**

   - Respeita limites semÃ¢nticos (nÃ£o corta listas, citaÃ§Ãµes, blocos de cÃ³digo)
   - Overlap baseado em sentenÃ§as completas
   - ProteÃ§Ã£o contra recursÃ£o infinita
   - MÃ­nimo de chunk size configurÃ¡vel

2. **Suporte a MÃºltiplos Formatos**

   - PDF (com rastreamento preciso de pÃ¡ginas)
   - CSV/Excel (com contexto de colunas/sheets)
   - Parquet (com schema)
   - TXT/Markdown

3. **DeduplicaÃ§Ã£o AutomÃ¡tica**

   - Hash SHA256 para identificaÃ§Ã£o Ãºnica
   - Remove chunks duplicados automaticamente

4. **Escolha AutomÃ¡tica de Ãndice**

   - FAISS Flat para volumes pequenos (< 10k chunks)
   - FAISS IVF para grandes volumes (â‰¥ 10k chunks)
   - ConfiguraÃ§Ã£o manual tambÃ©m disponÃ­vel

5. **Metadados Ricos**

   - Fonte, pÃ¡gina, tipo de documento
   - Timestamps de criaÃ§Ã£o e indexaÃ§Ã£o
   - Contexto do documento

6. **Logging Estruturado**
   - MÃ©tricas em JSON (JSONL)
   - Rastreamento de performance
   - AnÃ¡lise posterior facilitada

### Sistema RAG (`perguntar.py`)

#### âœ¨ Principais Features:

1. **Query Expansion**

   - Expande queries com sinÃ´nimos e termos relacionados
   - DicionÃ¡rio customizÃ¡vel
   - Melhora recall sem modelos pesados

2. **BM25 Reranking**

   - Reordena resultados vetoriais usando BM25
   - Sem modelos extras (apenas TF-IDF)
   - Ideal para ambientes com RAM limitada

3. **Busca Vetorial FAISS**

   - Busca eficiente por similaridade
   - Suporta Ã­ndices Flat e IVF
   - NormalizaÃ§Ã£o de embeddings

4. **GeraÃ§Ã£o de Respostas**

   - Usa contexto recuperado para gerar respostas
   - Metadados nas fontes citadas
   - Controle de alucinaÃ§Ã£o ("NÃ£o sei" quando apropriado)

5. **MÃ©tricas Detalhadas**
   - Tempo de retrieval, reranking e geraÃ§Ã£o
   - NÃºmero de documentos usados
   - Scores de similaridade e BM25
   - Logging em JSON para anÃ¡lise

## âš™ï¸ ConfiguraÃ§Ãµes AvanÃ§adas

### ParÃ¢metros de IndexaÃ§Ã£o

```python
create_embeddings(
    documents=["..."],
    model_name="qwen3-embedding:4b",

    # Chunking
    chunk_size=1000,              # Tamanho mÃ¡ximo do chunk
    chunk_overlap=150,            # Overlap entre chunks
    min_chunk_size=50,            # Tamanho mÃ­nimo aceitÃ¡vel

    # Performance
    batch_size=512,               # Chunks por lote
    num_workers=4,                # Threads paralelas

    # Ãndice
    use_ivf_index=None,           # None=automÃ¡tico, True/False=manual
    ivf_nlist=100,                # Clusters para IVF
    ivf_threshold=10000,          # Threshold para ativar IVF

    # Qualidade
    normalize_embeddings=True,    # Normaliza vetores
    deduplicate=True,             # Remove duplicados
    add_document_context=True,    # Adiciona contexto do doc

    # Output
    output_prefix="meu_index",
    enable_structured_logging=True,
    custom_metadata={"projeto": "RAG", "versÃ£o": "1.0"}
)
```

### ParÃ¢metros de Query

```python
rag.query(
    question="Sua pergunta?",
    k=10,                 # Docs iniciais (busca vetorial)
    rerank_to=4          # Docs finais (apÃ³s BM25)
)
```

## ğŸ“ˆ MÃ©tricas e AnÃ¡lise

### Arquivo de MÃ©tricas (`*_metrics.jsonl`)

Exemplo de entrada:

```json
{
  "timestamp": "2025-11-04T08:31:33",
  "session_id": "a1b2c3d4",
  "event_type": "retrieval",
  "original_query": "Qual Ã© a data?",
  "expanded_query": "qual Ã© a data do primeiro registro",
  "num_results": 3,
  "retrieval_time_ms": 820,
  "rerank_time_ms": 5,
  "vector_avg_score": 0.7234,
  "bm25_avg_score": 12.45
}
```

### AnÃ¡lise de MÃ©tricas

```python
import json
import pandas as pd

# Carrega mÃ©tricas
metrics = []
with open("meu_index_metrics.jsonl", "r") as f:
    for line in f:
        metrics.append(json.loads(line))

df = pd.DataFrame(metrics)

# AnÃ¡lise
print(f"Tempo mÃ©dio de retrieval: {df['retrieval_time_ms'].mean():.2f}ms")
print(f"Tempo mÃ©dio de reranking: {df['rerank_time_ms'].mean():.2f}ms")
print(f"Score vetorial mÃ©dio: {df['vector_avg_score'].mean():.4f}")
```

## ğŸ”§ Troubleshooting

### Erro: "Modelo nÃ£o encontrado"

```bash
# Liste modelos disponÃ­veis
ollama list

# Baixe o modelo necessÃ¡rio
ollama pull qwen3-embedding:4b
ollama pull qwen3:latest
```

### Erro: "Chunk size muito grande"

Reduza o `chunk_size` para caber no contexto do modelo:

```python
chunk_size=800,  # Reduzido de 1000
```

### Performance lenta

1. Aumente `num_workers` (paralelizaÃ§Ã£o)
2. Aumente `batch_size` (menos lotes)
3. Use Ã­ndice IVF para grandes volumes
4. Considere modelo de embedding menor

### Respostas de baixa qualidade

1. Aumente `k` (mais documentos recuperados)
2. Ajuste `chunk_overlap` (melhor contexto)
3. Use reranking (`use_reranking=True`)
4. Ative query expansion (`use_query_expansion=True`)
5. Teste diferentes modelos LLM

## ğŸ“š Exemplos de Uso

### Indexar MÃºltiplos Documentos

```python
result = create_embeddings(
    documents=[
        "/docs/manual.pdf",
        "/docs/relatorio.xlsx",
        "/docs/dados.csv",
        "/docs/notas/",  # DiretÃ³rio inteiro
    ],
    model_name="qwen3-embedding:4b",
    output_prefix="conhecimento_base",
)
```

### Query Customizada

```python
from perguntar import AdvancedRAG

rag = AdvancedRAG(
    index_path="conhecimento_base.faiss",
    metadata_path="conhecimento_base.jsonl",
    embedding_model="qwen3-embedding:4b",
    llm_model="qwen3:latest",
    use_reranking=True,
    use_query_expansion=True,
)

result = rag.query(
    question="Como configurar o sistema?",
    k=15,
    rerank_to=5
)

print(result["answer"])
```

### Adicionar ExpansÃµes Customizadas

```python
from perguntar import QueryExpander

expander = QueryExpander()
expander.add_custom_expansion("RAG", ["retrieval", "augmented", "generation"])
expander.add_custom_expansion("AI", ["inteligÃªncia artificial", "machine learning", "deep learning"])

query_expandida = expander.expand("Como usar RAG com AI?")
print(query_expandida)
```

## ğŸ¯ Boas PrÃ¡ticas

1. **Chunking**

   - Use `chunk_size` entre 500-1500 caracteres
   - `chunk_overlap` entre 10-20% do chunk_size
   - `min_chunk_size` â‰ˆ 5-10% do chunk_size

2. **IndexaÃ§Ã£o**

   - Sempre use `deduplicate=True`
   - Ative `normalize_embeddings=True`
   - Use `add_document_context=True` para melhor rastreabilidade

3. **Query**

   - Comece com `k=10` e `rerank_to=3-5`
   - Ative reranking para melhor precisÃ£o
   - Use query expansion para melhor recall

4. **Performance**
   - Monitore mÃ©tricas regularmente
   - Ajuste `num_workers` baseado em CPU
   - Use IVF para > 10k chunks

## ğŸ“ LicenÃ§a

Este sistema RAG Ã© parte do projeto AI_Agent.
