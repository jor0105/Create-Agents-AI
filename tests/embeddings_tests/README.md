# ğŸš€ Pipeline Modular de Embeddings com Ollama - VersÃ£o Profissional

Sistema extremamente simples e modular para criar embeddings de documentos usando modelos do Ollama, com **detecÃ§Ã£o automÃ¡tica de Ã­ndice** e melhores prÃ¡ticas da indÃºstria.

## âœ¨ CaracterÃ­sticas

- âœ… **Ultra Simples**: Uma Ãºnica funÃ§Ã£o para tudo
- âœ… **Totalmente Modular**: Customize qualquer parÃ¢metro
- âœ… **MÃºltiplos Formatos**: PDF, CSV, Parquet, Excel, TXT, MD
- âœ… **Processamento Paralelo**: Aproveita mÃºltiplos cores
- âœ… **Seguro para Notebooks**: Logs detalhados e tratamento de erros
- âœ… **Eficiente em MemÃ³ria**: Streaming de dados grandes
- ğŸ†• **DetecÃ§Ã£o AutomÃ¡tica de Ãndice**: Escolhe Flat ou IVF baseado no volume
- ğŸ†• **NormalizaÃ§Ã£o de Embeddings**: Para similaridade coseno eficiente
- ğŸ†• **DeduplicaÃ§Ã£o AutomÃ¡tica**: Economiza 20-40% de espaÃ§o
- ğŸ†• **Metadados Profissionais**: Rastreamento completo (15+ campos)

## ğŸ“¦ InstalaÃ§Ã£o

```bash
pip install ollama faiss-cpu numpy pandas pyarrow pymupdf langchain-text-splitters openpyxl
```

## ğŸ¯ Uso RÃ¡pido

### Exemplo BÃ¡sico (com DetecÃ§Ã£o AutomÃ¡tica)

```python
from indexar import create_embeddings

# O sistema decide automaticamente o melhor Ã­ndice!
result = create_embeddings(
    documents=["./meus_documentos"],  # DiretÃ³rio ou lista de arquivos
    model_name="qwen3-embedding:4b"   # Modelo do Ollama
)

# < 10.000 chunks â†’ IndexFlatL2 (busca exata)
# â‰¥ 10.000 chunks â†’ IndexIVFFlat (busca aproximada, escalÃ¡vel)

print(f"âœ“ {result['total_chunks']} chunks indexados")
print(f"ğŸ”§ Ãndice: {result['index_type']} ({'automÃ¡tico' if result['index_auto_selected'] else 'manual'})")
print(f"ğŸ—‘ï¸  Duplicados removidos: {result['duplicates_removed']}")
```

### Exemplo com Arquivos EspecÃ­ficos

```python
result = create_embeddings(
    documents=[
        "artigo1.pdf",
        "dados.csv",
        "relatorio.parquet",
        "planilha.xlsx"
    ],
    model_name="qwen3-embedding:4b",
    chunk_size=800,
    chunk_overlap=100,
    output_prefix="arquivos_especificos"
)
```

### Exemplo Completo com Todas as OpÃ§Ãµes

````python
result = create_embeddings(
    # Documentos
    documents=["./docs_tecnicos", "./relatorios"],

    # Modelo de IA
    model_name="qwen3-embedding:4b",

    # ConfiguraÃ§Ã£o de chunking
    chunk_size=1200,
    chunk_overlap=200,
    min_chunk_size=50,  # âœ… Ignora chunks muito pequenos

    # Qualidade RAG (RECOMENDADO - sempre True)
    normalize_embeddings=True,  # âœ… Para similaridade coseno
    deduplicate=True,  # âœ… Remove duplicados (-20 a -40%)
    add_document_context=True,  # âœ… Metadados ricos

    # Ãndice FAISS (detecÃ§Ã£o automÃ¡tica por padrÃ£o)
    # use_ivf_index=None (padrÃ£o) - Sistema decide automaticamente
    # use_ivf_index=True - ForÃ§a IVF
    # use_ivf_index=False - ForÃ§a Flat
    ivf_threshold=10000,  # Threshold para ativar IVF

    # Metadados customizados
    custom_metadata={
        "project": "RAG System",
        "version": "2.0"
    },

    # Performance
    batch_size=512,
    num_workers=4,
    output_prefix="indice_completo"
)
```## ğŸ“‹ ParÃ¢metros

| ParÃ¢metro | Tipo | PadrÃ£o | DescriÃ§Ã£o |
|-----------|------|--------|-----------|
| `documents` | List[str] | **ObrigatÃ³rio** | Arquivos ou diretÃ³rios para processar |
| `model_name` | str | "qwen3-embedding:4b" | Modelo Ollama para embeddings |
| `chunk_size` | int | 1000 | Tamanho mÃ¡ximo de cada chunk (caracteres) |
| `chunk_overlap` | int | 150 | SobreposiÃ§Ã£o entre chunks (caracteres) |
| `batch_size` | int | 512 | Chunks a processar por vez |
| `num_workers` | int | 4 | NÃºmero de threads paralelas |
| `output_prefix` | str | "vector_index" | Prefixo dos arquivos de saÃ­da |
| `normalize_embeddings` | bool | True | âœ… Normaliza vetores (RECOMENDADO) |
| `deduplicate` | bool | True | âœ… Remove duplicados (RECOMENDADO) |
| `min_chunk_size` | int | 50 | Tamanho mÃ­nimo de chunk |
| `add_document_context` | bool | True | Adiciona contexto do documento |
| `use_ivf_index` | bool\|None | None | None=automÃ¡tico, True=IVF, False=Flat |
| `ivf_threshold` | int | 10000 | Chunks para ativar IVF automaticamente |
| `ivf_nlist` | int | 100 | NÃºmero de clusters IVF |
| `custom_metadata` | Dict | None | Metadados customizados (JSON) |

## ğŸ“„ Formatos Suportados

- **PDF** (.pdf) - Documentos de texto (com rastreamento de pÃ¡ginas)
- **CSV** (.csv) - Planilhas de dados
- **Parquet** (.parquet) - Dados colunares
- **Excel** (.xlsx, .xls) - Planilhas do Microsoft Excel
- **Texto** (.txt) - Arquivos de texto simples
- **Markdown** (.md) - Documentos Markdown

## ğŸ†• DetecÃ§Ã£o AutomÃ¡tica de Ãndice FAISS

O sistema agora escolhe **automaticamente** o melhor tipo de Ã­ndice baseado no volume de documentos:

| Volume de Chunks | Ãndice Selecionado | CaracterÃ­sticas |
|------------------|-------------------|-----------------|
| **< 10.000** | IndexFlatL2 | âœ… Busca exata, muito rÃ¡pida |
| **â‰¥ 10.000** | IndexIVFFlat | âœ… Busca aproximada, escalÃ¡vel (10-100x mais rÃ¡pido) |

**VocÃª nÃ£o precisa se preocupar!** O sistema otimiza automaticamente.

### Controlando Manualmente (se necessÃ¡rio):

```python
# DetecÃ§Ã£o automÃ¡tica (RECOMENDADO)
result = create_embeddings(documents=["./docs"])

# ForÃ§ar Flat (busca exata)
result = create_embeddings(documents=["./docs"], use_ivf_index=False)

# ForÃ§ar IVF (grande escala)
result = create_embeddings(documents=["./docs"], use_ivf_index=True)

# Ajustar threshold (padrÃ£o: 10.000)
result = create_embeddings(documents=["./docs"], ivf_threshold=15000)
````

## ğŸ¨ Modelos do Ollama Recomendados

```python
# Para portuguÃªs (recomendado)
model_name="qwen3-embedding:4b"

# Alternativas
model_name="llama2"
model_name="nomic-embed-text"
model_name="mxbai-embed-large"
```

## âš™ï¸ OtimizaÃ§Ã£o de Performance

### MÃ¡xima Velocidade (usa mais recursos)

```python
batch_size=1024      # Lotes grandes
num_workers=16       # Muitas threads
```

### MÃ­nimo de Recursos (mais lento, mas seguro)

```python
batch_size=128       # Lotes pequenos
num_workers=2        # Poucas threads
```

### Balanceado (recomendado)

```python
batch_size=512       # Lotes mÃ©dios
num_workers=4        # Threads moderadas
```

## ğŸ“Š Chunks por Tipo de Documento

### Documentos TÃ©cnicos/CientÃ­ficos

```python
chunk_size=1500      # Chunks grandes
chunk_overlap=250    # Overlap maior
```

### Dados Tabulares (CSV/Excel)

```python
chunk_size=500       # Chunks pequenos
chunk_overlap=50     # Overlap mÃ­nimo
```

### Documentos Gerais

```python
chunk_size=1000      # Tamanho mÃ©dio
chunk_overlap=150    # Overlap mÃ©dio
```

## ğŸ“‚ Arquivos Gerados

ApÃ³s o processamento, dois arquivos sÃ£o criados:

1. **`[output_prefix].faiss`** - Ãndice vetorial FAISS para busca
2. **`[output_prefix].jsonl`** - Metadados de cada chunk

## ğŸ’¡ Exemplos PrÃ¡ticos

### 1. Processar diretÃ³rio inteiro

```python
result = create_embeddings(
    documents=["./documentos"],
    model_name="qwen3-embedding:4b"
)
```

### 2. MÃºltiplas fontes

```python
result = create_embeddings(
    documents=[
        "./artigos",
        "./relatorios",
        "documento_importante.pdf"
    ],
    model_name="qwen3-embedding:4b"
)
```

### 3. Usar em Jupyter Notebook

```python
# Os logs aparecem automaticamente mostrando progresso
result = create_embeddings(
    documents=["./dados"],
    output_prefix="notebook_index"
)

# Resultados disponÃ­veis para anÃ¡lise
print(f"Chunks: {result['total_chunks']}")
print(f"Tempo: {result['time_seconds']}s")
```

## ğŸ”§ Uso via Linha de Comando

```bash
# Processar documentos via CLI
python indexar.py ./documentos --model qwen3-embedding:4b --chunk-size 1000 --output meu_indice

# Ver todas as opÃ§Ãµes
python indexar.py --help
```

## ğŸ“ Arquivos no Projeto

- **`indexar.py`** - MÃ³dulo principal com a funÃ§Ã£o `create_embeddings()`
- **`exemplo_uso.py`** - Exemplos de uso em scripts Python
- **`exemplo_embeddings.ipynb`** - Notebook interativo com exemplos
- **`indexar_antigo.py`** - VersÃ£o anterior (backup)

## ğŸš€ ComeÃ§ando RÃ¡pido

1. Instale as dependÃªncias
2. Garanta que o Ollama estÃ¡ rodando
3. Importe e use:

```python
from indexar import create_embeddings

result = create_embeddings(
    documents=["./meus_docs"],
    model_name="qwen3-embedding:4b"
)
```

Pronto! Seus embeddings estÃ£o criados.

## ğŸ¤ IntegraÃ§Ã£o com RAG

Os arquivos gerados podem ser usados diretamente em sistemas RAG:

```python
import faiss
import json

# Carregar Ã­ndice
index = faiss.read_index("meu_indice.faiss")

# Carregar metadados
metadata = []
with open("meu_indice.jsonl", "r") as f:
    for line in f:
        metadata.append(json.loads(line))

# Fazer busca
query_vector = [...]  # Seu vetor de consulta
distances, indices = index.search(query_vector, k=5)

# Recuperar chunks relevantes
for idx in indices[0]:
    print(metadata[idx])
```

## ğŸ“– DocumentaÃ§Ã£o Adicional

- **Ollama**: https://ollama.ai
- **FAISS**: https://github.com/facebookresearch/faiss
- **LangChain**: https://python.langchain.com

## ğŸ› SoluÃ§Ã£o de Problemas

### "Prompt too long"

Reduza o `chunk_size`:

```python
chunk_size=800  # ou 500
```

### Processamento muito lento

Aumente `num_workers` e `batch_size`:

```python
num_workers=8
batch_size=1024
```

### Usando muita RAM

Reduza `batch_size`:

```python
batch_size=128
```

## ğŸ“„ LicenÃ§a

Este cÃ³digo Ã© fornecido como estÃ¡ para uso educacional.
